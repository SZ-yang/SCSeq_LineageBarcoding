{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    " (64)\n",
    "# Instantiate multi-head attention\n",
    "mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "# Example input tensor: (batch, seq_len, embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Self-attention: Q=K=V=x\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(output.shape)        # [32, 10, 128]\n",
    "print(attn_weights.shape)  # [32, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the combined in-projection weights\n",
    "combined_weight = mha.in_proj_weight  # Shape: (3*embed_dim, embed_dim)\n",
    "\n",
    "# Split them into Q, K, V weight matrices\n",
    "q_weight = combined_weight[:embed_dim, :]\n",
    "k_weight = combined_weight[embed_dim:2*embed_dim, :]\n",
    "v_weight = combined_weight[2*embed_dim:, :]\n",
    "\n",
    "print(\"Query Weight Matrix shape:\", q_weight.shape)\n",
    "print(\"Key Weight Matrix shape:\", k_weight.shape)\n",
    "print(\"Value Weight Matrix shape:\", v_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCellEncoder(nn.Module):\n",
    "    def __init__(self, num_genes=2000, embed_dim=128, num_heads=8, hidden_dim=256, final_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 0: Gene embedding\n",
    "        self.input_embedding = nn.Linear(num_genes, embed_dim)\n",
    "\n",
    "        # Step 1: Multi-head self-attention\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "        # Step 2 & 4: Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Step 3: Feed-forward\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        # Step 5: Final linear reduction\n",
    "        self.final_linear = nn.Linear(embed_dim, final_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_cells, num_genes)\n",
    "\n",
    "        # Embedding genes into lower-dimension\n",
    "        x_emb = self.input_embedding(x)  # (batch_size, num_cells, embed_dim)\n",
    "\n",
    "        # Multi-head self-attention (across cells)\n",
    "        attn_output, attn_weights = self.mha(query=x_emb, key=x_emb, value=x_emb)\n",
    "\n",
    "        # Add & Norm (Transformer style)\n",
    "        x = self.norm1(x_emb + attn_output)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_output = self.ff(x)\n",
    "\n",
    "        # Add & Norm again\n",
    "        x = self.norm2(x + ff_output)\n",
    "\n",
    "        # Reduce to final embedding\n",
    "        final_embedding = self.final_linear(x)  # (batch_size, num_cells, final_dim)\n",
    "\n",
    "        return final_embedding  # Ready to pass to your projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Simulate input ===\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 4\n",
    "cells_per_lineage = 3\n",
    "embedding_dim = 2\n",
    "\n",
    "features = torch.randn(batch_size, cells_per_lineage, embedding_dim)\n",
    "print(\"Simulated features shape:\", features.shape)\n",
    "\n",
    "# # === Run loss ===\n",
    "# loss_fn = ContrastiveLoss(temperature=0.5)\n",
    "# loss = loss_fn(features)\n",
    "# print(\"Contrastive loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1.1258/(((-1.1258)**2+(-1.1524)**2)**(1/2)), -1.1524/(((-1.1258)**2+(-1.1524)**2)**(1/2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, cells_per_lineage, embedding_dim = features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = F.normalize(features, dim=2)\n",
    "print(\"features:\", features)\n",
    "features_flat = features.view(batch_size * cells_per_lineage, embedding_dim)\n",
    "print(\"features_flat:\", features_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = torch.matmul(features_flat, features_flat.T) / temperature\n",
    "similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange(batch_size).repeat_interleave(cells_per_lineage).to(features.device)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.eye(batch_size * cells_per_lineage, dtype=torch.bool).to(features.device)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = F.log_softmax(logits, dim=1)\n",
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_mask = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "positive_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_mask = positive_mask & (~mask)\n",
    "positive_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count = positive_mask.sum(1)\n",
    "positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = loss.mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -(positive_mask * log_prob).sum(1) / positive_count.clamp(min=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_mask * log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(positive_mask * log_prob).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.exp(-math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        \"\"\"\n",
    "        Contrastive Loss for supervised contrastive learning on cell embeddings.\n",
    "\n",
    "        Args:\n",
    "            temperature (float): Scaling factor for similarity scores.\n",
    "        \"\"\"\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Compute the supervised contrastive loss for the provided features.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): Embeddings of shape\n",
    "                                     (batch_size, cells_per_lineage, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Scalar contrastive loss.\n",
    "        \"\"\"\n",
    "        batch_size, cells_per_lineage, embedding_dim = features.shape\n",
    "\n",
    "        # NaN check before anything\n",
    "        if torch.isnan(features).any():\n",
    "            print(\"[DEBUG] NaN detected in input features!\")\n",
    "            print(\"features:\", features)\n",
    "            exit()\n",
    "\n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=2)\n",
    "\n",
    "        # Reshape to (batch_size * cells_per_lineage, embedding_dim)\n",
    "        features_flat = features.view(batch_size * cells_per_lineage, embedding_dim)\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(features_flat, features_flat.T) / self.temperature\n",
    "\n",
    "        # Create labels indicating positive pairs (cells from same lineage)\n",
    "        labels = torch.arange(batch_size).repeat_interleave(cells_per_lineage).to(features.device)\n",
    "\n",
    "        # Mask to exclude self-comparisons\n",
    "        mask = torch.eye(batch_size * cells_per_lineage, dtype=torch.bool).to(features.device)\n",
    "\n",
    "        # Compute log-softmax of similarities\n",
    "        logits = similarity_matrix.masked_fill(mask, float('-inf'))\n",
    "        log_prob = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        # Create mask for positive pairs (same lineage but not the same cell)\n",
    "        positive_mask = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        positive_mask = positive_mask & (~mask)  # remove self-pairs\n",
    "\n",
    "        # Count positive pairs for normalization\n",
    "        positive_count = positive_mask.sum(1)\n",
    "\n",
    "        # Compute loss (only for cells with at least one positive pair)\n",
    "        masked_log_prob = log_prob.masked_fill(~positive_mask, 0.0)\n",
    "        loss = -masked_log_prob.sum(1) / positive_count.clamp(min=1)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Post-computation check\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"[DEBUG] NaN or Inf detected in loss!\")\n",
    "            print(\"loss:\", loss)\n",
    "            print(\"features_flat (sample):\", features_flat[0])\n",
    "            print(\"similarity_matrix (sample):\", similarity_matrix[0][:10])\n",
    "            print(\"log_prob (sample):\", log_prob[0][:10])\n",
    "            exit()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated features shape: torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# === Simulate input ===\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 4\n",
    "cells_per_lineage = 3\n",
    "embedding_dim = 2\n",
    "\n",
    "features = torch.randn(batch_size, cells_per_lineage, embedding_dim)\n",
    "print(\"Simulated features shape:\", features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive loss: 3.346036911010742\n"
     ]
    }
   ],
   "source": [
    "# === Run loss ===\n",
    "loss_fn = ContrastiveLoss(temperature=0.5)\n",
    "loss = loss_fn(features)\n",
    "print(\"Contrastive loss:\", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

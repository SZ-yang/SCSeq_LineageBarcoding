{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37207, 2000) (3886, 2000)\n",
      "(37207, 64) (3886, 64)\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/Users/apple/Desktop/KB/data\"\n",
    "adata_train = ad.read_h5ad(input_dir+'/LarryData/train_test/Larry_train.h5ad')\n",
    "adata_test = ad.read_h5ad(input_dir+'/LarryData/train_test/Larry_test.h5ad')\n",
    "\n",
    "train_labels = adata_train.obs[\"clone_id\"].to_numpy()\n",
    "test_labels = adata_test.obs[\"clone_id\"].to_numpy()\n",
    "\n",
    " \n",
    "X_train = np.load(input_dir+'/feat_RECOMB/train_test/larry_train_test/Larry_LCL_train_embeddings.npy')\n",
    "X_test = np.load(input_dir+'/feat_RECOMB/train_test/larry_train_test/Larry_LCL_test_embeddings.npy')\n",
    "\n",
    "print(adata_train.shape, adata_test.shape)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the cells in the neutrophil monocyte trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_df.shape: (130887, 8) ; cell_id.shape: (96373, 1)\n",
      "filtered_meta_df.shape:  (96373, 9)\n",
      "37207 3886\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "meta_df = pd.read_csv(\"/Users/apple/Desktop/KB/Dataset1/stateFate_inVitro_metadata.txt.gz\", sep='\\t')\n",
    "cell_id = pd.read_csv(\"/Users/apple/Desktop/KB/Dataset1/stateFate_inVitro_neutrophil_monocyte_trajectory.txt.gz\", sep='\\t')\n",
    "print(\"meta_df.shape:\" ,meta_df.shape, \"; cell_id.shape:\", cell_id.shape )\n",
    "\n",
    "# find the cells \n",
    "cell_indices = cell_id['Cell index']\n",
    "\n",
    "# Use these indices to select rows from 'meta_df'\n",
    "filtered_meta_df = meta_df.loc[cell_indices].copy()\n",
    "filtered_meta_df[\"Lib_Cellbarcode\"] = filtered_meta_df['Library'].astype(str) + \"_\" + filtered_meta_df['Cell barcode'].astype(str)\n",
    "\n",
    "# Display the filtered dataframe\n",
    "print(\"filtered_meta_df.shape: \", filtered_meta_df.shape)\n",
    "\n",
    "# fiter adata and embedding\n",
    "adata_train.obs[\"Lib_Cellbarcode\"] = adata_train.obs['Library'].astype(str) + \"_\" + adata_train.obs['Cell barcode'].astype(str)\n",
    "adata_test.obs[\"Lib_Cellbarcode\"] = adata_test.obs['Library'].astype(str) + \"_\" + adata_test.obs['Cell barcode'].astype(str)\n",
    "\n",
    "print(len(adata_train.obs[\"Lib_Cellbarcode\"].unique()), len(adata_test.obs[\"Lib_Cellbarcode\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "len(shared_barcodes_train):  26432\n",
      "***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((26432, 2000), (26432, 64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Find the shared 'Lib_Cellbarcode' values\n",
    "shared_barcodes_train = np.intersect1d(filtered_meta_df['Lib_Cellbarcode'], adata_train.obs['Lib_Cellbarcode'])\n",
    "print(\"***\")\n",
    "print(\"len(shared_barcodes_train): \", len(shared_barcodes_train))\n",
    "print(\"***\")\n",
    "\n",
    "# Step 2: Filter 'adata_train' based on the shared barcodes\n",
    "adata_train_filter = adata_train[adata_train.obs['Lib_Cellbarcode'].isin(shared_barcodes_train)].copy()\n",
    "\n",
    "# Step 3: Filter 'X_train' based on the same shared barcodes\n",
    "# Find the indices of the shared barcodes in 'adata_train.obs'\n",
    "indices = adata_train.obs['Lib_Cellbarcode'].isin(shared_barcodes_train).values\n",
    "\n",
    "# Use these indices to filter 'X_train'\n",
    "X_train_filter = X_train[indices]\n",
    "adata_train_filter.shape, X_train_filter.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "len(shared_barcodes_test):  2705\n",
      "***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2705, 2000), (2705, 64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Find the shared 'Lib_Cellbarcode' values\n",
    "shared_barcodes_test = np.intersect1d(filtered_meta_df['Lib_Cellbarcode'], adata_test.obs['Lib_Cellbarcode'])\n",
    "print(\"***\")\n",
    "print(\"len(shared_barcodes_test): \", len(shared_barcodes_test))\n",
    "print(\"***\")\n",
    "\n",
    "# Step 2: Filter 'adata_train' based on the shared barcodes\n",
    "adata_test_filter = adata_test[adata_test.obs['Lib_Cellbarcode'].isin(shared_barcodes_test)].copy()\n",
    "\n",
    "# Step 3: Filter 'X_train' based on the same shared barcodes\n",
    "# Find the indices of the shared barcodes in 'adata_train.obs'\n",
    "indices = adata_test.obs['Lib_Cellbarcode'].isin(shared_barcodes_test).values\n",
    "\n",
    "# Use these indices to filter 'X_train'\n",
    "X_test_filter = X_test[indices]\n",
    "adata_test_filter.shape, X_test_filter.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Composition pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composit_pair_gen(X_train, adata_train):\n",
    "    # train_labels = adata_train.obs[\"clone_id\"].to_numpy()\n",
    "    # print(\"train_labels.shape:\", train_labels.shape)\n",
    "\n",
    "    ### generate the labels\n",
    "    adata_6 = adata_train[adata_train.obs[\"time_info\"] == 6.0]\n",
    "    print(\"adata_6.shape:\", adata_6.shape)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cell type distributions\n",
    "    clone_state_info_distribution = {}\n",
    "\n",
    "    # Get the unique lineage\n",
    "    unique_clone_ids = adata_6.obs[\"clone_id\"].unique()\n",
    "\n",
    "    # Loop through each unique lineage\n",
    "    for clone_id in unique_clone_ids:\n",
    "        # Filter the data to get only rows with the current clone_id\n",
    "        clone_data = adata_6.obs[adata_6.obs[\"clone_id\"] == clone_id]\n",
    "        \n",
    "        # Get the distribution of cell types in the current clone_id\n",
    "        state_info_distribution = clone_data[\"state_info\"].value_counts(normalize=True)\n",
    "        \n",
    "        # Round each percentage to 4 decimal places and convert to a dictionary\n",
    "        state_info_distribution = state_info_distribution.round(4).to_dict()\n",
    "        \n",
    "        # Store the rounded distribution in the main dictionary\n",
    "        clone_state_info_distribution[clone_id] = state_info_distribution\n",
    "\n",
    "    # Print the resulting dictionary for verification\n",
    "    i = 0\n",
    "    for clone_id, distribution in clone_state_info_distribution.items():\n",
    "        print(f\"Clone ID: {clone_id}, Cell Type Distribution: {distribution}\")\n",
    "        i+=1\n",
    "        if i ==3:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Step 1: Get embeddings for Day 12 cells\n",
    "    day2_mask = adata_train.obs[\"time_info\"] == 2.0\n",
    "    X_train_day2 = X_train[day2_mask.values] \n",
    "    print(f\"Day 12 embeddings shape: {X_train_day2.shape}\")\n",
    "\n",
    "    # Step 2: Get the clone labels for Day 12 cells\n",
    "    clone_labels_day2 = adata_train.obs.loc[day2_mask, \"clone_id\"].to_numpy()\n",
    "\n",
    "    # Step 3: Initialize y_train_prob matrix to store the probabilities\n",
    "    # n_classes = len(adata_train.obs[\"state_info\"].unique())\n",
    "    y_train_prob = np.zeros((X_train_day2.shape[0], 3))\n",
    "\n",
    "    # Step 4: Assign the distributions from clone_state_info_distribution to each cell based on its clone_id\n",
    "    for i, clone_id in enumerate(clone_labels_day2):\n",
    "        if clone_id in clone_state_info_distribution:\n",
    "            # Get the distribution for the clone\n",
    "            distribution = clone_state_info_distribution[clone_id]\n",
    "            \n",
    "            # Ensure the order of cell types matches 'Undifferentiated', 'Monocyte', 'Neutrophil', 'Erythroid'\n",
    "            y_train_prob[i, 0] = distribution.get('Undifferentiated', 0)  # Default to 0 if not present\n",
    "            y_train_prob[i, 1] = distribution.get('Monocyte', 0)  # Default to 0 if not present\n",
    "            y_train_prob[i, 2] = distribution.get('Neutrophil', 0)  # Default to 0 if not present\n",
    "            # y_train_prob[i, 3] = distribution.get('Erythroid', 0)\n",
    "\n",
    "            # y_train_prob[i, 0] = distribution.get('iEP', 0)  # Default to 0 if not present\n",
    "            # y_train_prob[i, 1] = distribution.get('Ambiguous', 0)  # Default to 0 if not present\n",
    "            # y_train_prob[i, 2] = distribution.get('Fibroblast', 0)  # Default to 0 if not present\n",
    "\n",
    "    # Print the shape and first few examples of y_train_prob\n",
    "    print(f\"y_train_prob shape: {y_train_prob.shape}\")\n",
    "    print(f\"First 5 rows of y_train_prob:\\n{y_train_prob[:5]}\")\n",
    "\n",
    "\n",
    "    X_train_day2 = torch.tensor(X_train_day2, dtype=torch.float32)\n",
    "\n",
    "    # Example soft labels: 5 samples, each with a probability distribution over 3 classes\n",
    "    y_train_prob = torch.tensor(y_train_prob, dtype=torch.float32)\n",
    "\n",
    "    return X_train_day2, y_train_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test_filter.obs[\"state_info\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adata_6.shape: (16670, 2000)\n",
      "Clone ID: 1261, Cell Type Distribution: {'Neutrophil': 0.6957, 'Monocyte': 0.1957, 'Undifferentiated': 0.1087, 'Erythroid': 0.0}\n",
      "Clone ID: 2370, Cell Type Distribution: {'Monocyte': 0.4796, 'Neutrophil': 0.3061, 'Undifferentiated': 0.2143, 'Erythroid': 0.0}\n",
      "Clone ID: 292, Cell Type Distribution: {'Monocyte': 0.7073, 'Neutrophil': 0.1707, 'Undifferentiated': 0.122, 'Erythroid': 0.0}\n",
      "Day 12 embeddings shape: (1337, 64)\n",
      "y_train_prob shape: (1337, 3)\n",
      "First 5 rows of y_train_prob:\n",
      "[[0.1087 0.1957 0.6957]\n",
      " [0.0361 0.3253 0.6386]\n",
      " [0.0753 0.172  0.7527]\n",
      " [0.0753 0.172  0.7527]\n",
      " [0.2029 0.4203 0.3768]]\n",
      "adata_6.shape: (1841, 2000)\n",
      "Clone ID: 1261, Cell Type Distribution: {'Neutrophil': 0.5294, 'Undifferentiated': 0.2941, 'Monocyte': 0.1765}\n",
      "Clone ID: 2370, Cell Type Distribution: {'Monocyte': 0.6667, 'Undifferentiated': 0.2222, 'Neutrophil': 0.1111}\n",
      "Clone ID: 292, Cell Type Distribution: {'Monocyte': 0.7, 'Undifferentiated': 0.2, 'Neutrophil': 0.1}\n",
      "Day 12 embeddings shape: (69, 64)\n",
      "y_train_prob shape: (69, 3)\n",
      "First 5 rows of y_train_prob:\n",
      "[[0.     0.1429 0.8571]\n",
      " [0.3333 0.3333 0.3333]\n",
      " [0.     0.     1.    ]\n",
      " [0.     0.     1.    ]\n",
      " [0.6667 0.     0.3333]]\n"
     ]
    }
   ],
   "source": [
    "X_train_day2, y_train_prob = composit_pair_gen(X_train_filter, adata_train_filter)\n",
    "X_test_day2, y_test_prob = composit_pair_gen(X_test_filter, adata_test_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SoftLabelNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SoftLabelNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)  # Raw output before softmax\n",
    "        return out\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, X_train, y_train_prob, num_epochs=10000, lr=0.01):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.X_train = X_train\n",
    "        self.y_train_prob = y_train_prob\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Forward pass\n",
    "            outputs = self.model(self.X_train)\n",
    "            \n",
    "            # Apply log_softmax to get log probabilities\n",
    "            outputs_log_prob = torch.log_softmax(outputs, dim=1)\n",
    "            \n",
    "            # Calculate the KL divergence loss\n",
    "            loss = self.criterion(outputs_log_prob, self.y_train_prob)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Print loss every 50 epochs\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            # Apply softmax to get predicted probabilities\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "        return probabilities\n",
    "\n",
    "    def evaluate_kl_divergence(self, X_test, y_test_prob):\n",
    "        # Get the predicted log probabilities\n",
    "        predicted_probabilities_log = torch.log_softmax(self.model(X_test), dim=1)\n",
    "        \n",
    "        # Calculate KL divergence between predicted and true probabilities\n",
    "        kl_divergence = self.criterion(predicted_probabilities_log, y_test_prob)\n",
    "        return kl_divergence.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/70000], Loss: 0.1400\n",
      "Epoch [200/70000], Loss: 0.1108\n",
      "Epoch [300/70000], Loss: 0.0924\n",
      "Epoch [400/70000], Loss: 0.0782\n",
      "Epoch [500/70000], Loss: 0.0687\n",
      "Epoch [600/70000], Loss: 0.0628\n",
      "Epoch [700/70000], Loss: 0.0589\n",
      "Epoch [800/70000], Loss: 0.0563\n",
      "Epoch [900/70000], Loss: 0.0543\n",
      "Epoch [1000/70000], Loss: 0.0526\n",
      "Epoch [1100/70000], Loss: 0.0513\n",
      "Epoch [1200/70000], Loss: 0.0502\n",
      "Epoch [1300/70000], Loss: 0.0486\n",
      "Epoch [1400/70000], Loss: 0.0475\n",
      "Epoch [1500/70000], Loss: 0.0468\n",
      "Epoch [1600/70000], Loss: 0.0461\n",
      "Epoch [1700/70000], Loss: 0.0448\n",
      "Epoch [1800/70000], Loss: 0.0440\n",
      "Epoch [1900/70000], Loss: 0.0437\n",
      "Epoch [2000/70000], Loss: 0.0430\n",
      "Epoch [2100/70000], Loss: 0.0424\n",
      "Epoch [2200/70000], Loss: 0.0417\n",
      "Epoch [2300/70000], Loss: 0.0418\n",
      "Epoch [2400/70000], Loss: 0.0408\n",
      "Epoch [2500/70000], Loss: 0.0404\n",
      "Epoch [2600/70000], Loss: 0.0400\n",
      "Epoch [2700/70000], Loss: 0.0393\n",
      "Epoch [2800/70000], Loss: 0.0394\n",
      "Epoch [2900/70000], Loss: 0.0389\n",
      "Epoch [3000/70000], Loss: 0.0382\n",
      "Epoch [3100/70000], Loss: 0.0374\n",
      "Epoch [3200/70000], Loss: 0.0368\n",
      "Epoch [3300/70000], Loss: 0.0364\n",
      "Epoch [3400/70000], Loss: 0.0363\n",
      "Epoch [3500/70000], Loss: 0.0362\n",
      "Epoch [3600/70000], Loss: 0.0361\n",
      "Epoch [3700/70000], Loss: 0.0359\n",
      "Epoch [3800/70000], Loss: 0.0360\n",
      "Epoch [3900/70000], Loss: 0.0353\n",
      "Epoch [4000/70000], Loss: 0.0354\n",
      "Epoch [4100/70000], Loss: 0.0352\n",
      "Epoch [4200/70000], Loss: 0.0346\n",
      "Epoch [4300/70000], Loss: 0.0338\n",
      "Epoch [4400/70000], Loss: 0.0341\n",
      "Epoch [4500/70000], Loss: 0.0334\n",
      "Epoch [4600/70000], Loss: 0.0331\n",
      "Epoch [4700/70000], Loss: 0.0330\n",
      "Epoch [4800/70000], Loss: 0.0329\n",
      "Epoch [4900/70000], Loss: 0.0329\n",
      "Epoch [5000/70000], Loss: 0.0328\n",
      "Epoch [5100/70000], Loss: 0.0323\n",
      "Epoch [5200/70000], Loss: 0.0324\n",
      "Epoch [5300/70000], Loss: 0.0319\n",
      "Epoch [5400/70000], Loss: 0.0321\n",
      "Epoch [5500/70000], Loss: 0.0318\n",
      "Epoch [5600/70000], Loss: 0.0312\n",
      "Epoch [5700/70000], Loss: 0.0310\n",
      "Epoch [5800/70000], Loss: 0.0305\n",
      "Epoch [5900/70000], Loss: 0.0303\n",
      "Epoch [6000/70000], Loss: 0.0300\n",
      "Epoch [6100/70000], Loss: 0.0299\n",
      "Epoch [6200/70000], Loss: 0.0302\n",
      "Epoch [6300/70000], Loss: 0.0295\n",
      "Epoch [6400/70000], Loss: 0.0298\n",
      "Epoch [6500/70000], Loss: 0.0297\n",
      "Epoch [6600/70000], Loss: 0.0294\n",
      "Epoch [6700/70000], Loss: 0.0296\n",
      "Epoch [6800/70000], Loss: 0.0294\n",
      "Epoch [6900/70000], Loss: 0.0289\n",
      "Epoch [7000/70000], Loss: 0.0289\n",
      "Epoch [7100/70000], Loss: 0.0288\n",
      "Epoch [7200/70000], Loss: 0.0286\n",
      "Epoch [7300/70000], Loss: 0.0289\n",
      "Epoch [7400/70000], Loss: 0.0294\n",
      "Epoch [7500/70000], Loss: 0.0284\n",
      "Epoch [7600/70000], Loss: 0.0283\n",
      "Epoch [7700/70000], Loss: 0.0288\n",
      "Epoch [7800/70000], Loss: 0.0290\n",
      "Epoch [7900/70000], Loss: 0.0286\n",
      "Epoch [8000/70000], Loss: 0.0284\n",
      "Epoch [8100/70000], Loss: 0.0283\n",
      "Epoch [8200/70000], Loss: 0.0289\n",
      "Epoch [8300/70000], Loss: 0.0282\n",
      "Epoch [8400/70000], Loss: 0.0281\n",
      "Epoch [8500/70000], Loss: 0.0279\n",
      "Epoch [8600/70000], Loss: 0.0274\n",
      "Epoch [8700/70000], Loss: 0.0273\n",
      "Epoch [8800/70000], Loss: 0.0277\n",
      "Epoch [8900/70000], Loss: 0.0276\n",
      "Epoch [9000/70000], Loss: 0.0271\n",
      "Epoch [9100/70000], Loss: 0.0269\n",
      "Epoch [9200/70000], Loss: 0.0268\n",
      "Epoch [9300/70000], Loss: 0.0268\n",
      "Epoch [9400/70000], Loss: 0.0271\n",
      "Epoch [9500/70000], Loss: 0.0267\n",
      "Epoch [9600/70000], Loss: 0.0271\n",
      "Epoch [9700/70000], Loss: 0.0271\n",
      "Epoch [9800/70000], Loss: 0.0266\n",
      "Epoch [9900/70000], Loss: 0.0270\n",
      "Epoch [10000/70000], Loss: 0.0274\n",
      "Epoch [10100/70000], Loss: 0.0266\n",
      "Epoch [10200/70000], Loss: 0.0263\n",
      "Epoch [10300/70000], Loss: 0.0264\n",
      "Epoch [10400/70000], Loss: 0.0266\n",
      "Epoch [10500/70000], Loss: 0.0261\n",
      "Epoch [10600/70000], Loss: 0.0264\n",
      "Epoch [10700/70000], Loss: 0.0261\n",
      "Epoch [10800/70000], Loss: 0.0257\n",
      "Epoch [10900/70000], Loss: 0.0258\n",
      "Epoch [11000/70000], Loss: 0.0254\n",
      "Epoch [11100/70000], Loss: 0.0261\n",
      "Epoch [11200/70000], Loss: 0.0253\n",
      "Epoch [11300/70000], Loss: 0.0252\n",
      "Epoch [11400/70000], Loss: 0.0260\n",
      "Epoch [11500/70000], Loss: 0.0257\n",
      "Epoch [11600/70000], Loss: 0.0250\n",
      "Epoch [11700/70000], Loss: 0.0249\n",
      "Epoch [11800/70000], Loss: 0.0254\n",
      "Epoch [11900/70000], Loss: 0.0248\n",
      "Epoch [12000/70000], Loss: 0.0256\n",
      "Epoch [12100/70000], Loss: 0.0246\n",
      "Epoch [12200/70000], Loss: 0.0250\n",
      "Epoch [12300/70000], Loss: 0.0254\n",
      "Epoch [12400/70000], Loss: 0.0245\n",
      "Epoch [12500/70000], Loss: 0.0246\n",
      "Epoch [12600/70000], Loss: 0.0247\n",
      "Epoch [12700/70000], Loss: 0.0245\n",
      "Epoch [12800/70000], Loss: 0.0251\n",
      "Epoch [12900/70000], Loss: 0.0246\n",
      "Epoch [13000/70000], Loss: 0.0249\n",
      "Epoch [13100/70000], Loss: 0.0253\n",
      "Epoch [13200/70000], Loss: 0.0249\n",
      "Epoch [13300/70000], Loss: 0.0251\n",
      "Epoch [13400/70000], Loss: 0.0246\n",
      "Epoch [13500/70000], Loss: 0.0245\n",
      "Epoch [13600/70000], Loss: 0.0249\n",
      "Epoch [13700/70000], Loss: 0.0242\n",
      "Epoch [13800/70000], Loss: 0.0248\n",
      "Epoch [13900/70000], Loss: 0.0245\n",
      "Epoch [14000/70000], Loss: 0.0242\n",
      "Epoch [14100/70000], Loss: 0.0251\n",
      "Epoch [14200/70000], Loss: 0.0243\n",
      "Epoch [14300/70000], Loss: 0.0244\n",
      "Epoch [14400/70000], Loss: 0.0247\n",
      "Epoch [14500/70000], Loss: 0.0240\n",
      "Epoch [14600/70000], Loss: 0.0250\n",
      "Epoch [14700/70000], Loss: 0.0247\n",
      "Epoch [14800/70000], Loss: 0.0239\n",
      "Epoch [14900/70000], Loss: 0.0239\n",
      "Epoch [15000/70000], Loss: 0.0248\n",
      "Epoch [15100/70000], Loss: 0.0243\n",
      "Epoch [15200/70000], Loss: 0.0240\n",
      "Epoch [15300/70000], Loss: 0.0240\n",
      "Epoch [15400/70000], Loss: 0.0243\n",
      "Epoch [15500/70000], Loss: 0.0243\n",
      "Epoch [15600/70000], Loss: 0.0239\n",
      "Epoch [15700/70000], Loss: 0.0239\n",
      "Epoch [15800/70000], Loss: 0.0241\n",
      "Epoch [15900/70000], Loss: 0.0240\n",
      "Epoch [16000/70000], Loss: 0.0247\n",
      "Epoch [16100/70000], Loss: 0.0247\n",
      "Epoch [16200/70000], Loss: 0.0243\n",
      "Epoch [16300/70000], Loss: 0.0245\n",
      "Epoch [16400/70000], Loss: 0.0239\n",
      "Epoch [16500/70000], Loss: 0.0238\n",
      "Epoch [16600/70000], Loss: 0.0242\n",
      "Epoch [16700/70000], Loss: 0.0239\n",
      "Epoch [16800/70000], Loss: 0.0239\n",
      "Epoch [16900/70000], Loss: 0.0243\n",
      "Epoch [17000/70000], Loss: 0.0243\n",
      "Epoch [17100/70000], Loss: 0.0255\n",
      "Epoch [17200/70000], Loss: 0.0238\n",
      "Epoch [17300/70000], Loss: 0.0251\n",
      "Epoch [17400/70000], Loss: 0.0239\n",
      "Epoch [17500/70000], Loss: 0.0246\n",
      "Epoch [17600/70000], Loss: 0.0244\n",
      "Epoch [17700/70000], Loss: 0.0238\n",
      "Epoch [17800/70000], Loss: 0.0239\n",
      "Epoch [17900/70000], Loss: 0.0238\n",
      "Epoch [18000/70000], Loss: 0.0243\n",
      "Epoch [18100/70000], Loss: 0.0245\n",
      "Epoch [18200/70000], Loss: 0.0240\n",
      "Epoch [18300/70000], Loss: 0.0240\n",
      "Epoch [18400/70000], Loss: 0.0241\n",
      "Epoch [18500/70000], Loss: 0.0247\n",
      "Epoch [18600/70000], Loss: 0.0245\n",
      "Epoch [18700/70000], Loss: 0.0239\n",
      "Epoch [18800/70000], Loss: 0.0243\n",
      "Epoch [18900/70000], Loss: 0.0240\n",
      "Epoch [19000/70000], Loss: 0.0245\n",
      "Epoch [19100/70000], Loss: 0.0241\n",
      "Epoch [19200/70000], Loss: 0.0240\n",
      "Epoch [19300/70000], Loss: 0.0242\n",
      "Epoch [19400/70000], Loss: 0.0238\n",
      "Epoch [19500/70000], Loss: 0.0239\n",
      "Epoch [19600/70000], Loss: 0.0244\n",
      "Epoch [19700/70000], Loss: 0.0240\n",
      "Epoch [19800/70000], Loss: 0.0239\n",
      "Epoch [19900/70000], Loss: 0.0238\n",
      "Epoch [20000/70000], Loss: 0.0241\n",
      "Epoch [20100/70000], Loss: 0.0244\n",
      "Epoch [20200/70000], Loss: 0.0244\n",
      "Epoch [20300/70000], Loss: 0.0244\n",
      "Epoch [20400/70000], Loss: 0.0244\n",
      "Epoch [20500/70000], Loss: 0.0238\n",
      "Epoch [20600/70000], Loss: 0.0240\n",
      "Epoch [20700/70000], Loss: 0.0238\n",
      "Epoch [20800/70000], Loss: 0.0244\n",
      "Epoch [20900/70000], Loss: 0.0238\n",
      "Epoch [21000/70000], Loss: 0.0248\n",
      "Epoch [21100/70000], Loss: 0.0239\n",
      "Epoch [21200/70000], Loss: 0.0243\n",
      "Epoch [21300/70000], Loss: 0.0243\n",
      "Epoch [21400/70000], Loss: 0.0246\n",
      "Epoch [21500/70000], Loss: 0.0237\n",
      "Epoch [21600/70000], Loss: 0.0237\n",
      "Epoch [21700/70000], Loss: 0.0243\n",
      "Epoch [21800/70000], Loss: 0.0238\n",
      "Epoch [21900/70000], Loss: 0.0239\n",
      "Epoch [22000/70000], Loss: 0.0245\n",
      "Epoch [22100/70000], Loss: 0.0238\n",
      "Epoch [22200/70000], Loss: 0.0239\n",
      "Epoch [22300/70000], Loss: 0.0237\n",
      "Epoch [22400/70000], Loss: 0.0239\n",
      "Epoch [22500/70000], Loss: 0.0238\n",
      "Epoch [22600/70000], Loss: 0.0238\n",
      "Epoch [22700/70000], Loss: 0.0238\n",
      "Epoch [22800/70000], Loss: 0.0239\n",
      "Epoch [22900/70000], Loss: 0.0238\n",
      "Epoch [23000/70000], Loss: 0.0243\n",
      "Epoch [23100/70000], Loss: 0.0240\n",
      "Epoch [23200/70000], Loss: 0.0238\n",
      "Epoch [23300/70000], Loss: 0.0239\n",
      "Epoch [23400/70000], Loss: 0.0246\n",
      "Epoch [23500/70000], Loss: 0.0247\n",
      "Epoch [23600/70000], Loss: 0.0244\n",
      "Epoch [23700/70000], Loss: 0.0243\n",
      "Epoch [23800/70000], Loss: 0.0242\n",
      "Epoch [23900/70000], Loss: 0.0238\n",
      "Epoch [24000/70000], Loss: 0.0240\n",
      "Epoch [24100/70000], Loss: 0.0240\n",
      "Epoch [24200/70000], Loss: 0.0246\n",
      "Epoch [24300/70000], Loss: 0.0238\n",
      "Epoch [24400/70000], Loss: 0.0241\n",
      "Epoch [24500/70000], Loss: 0.0244\n",
      "Epoch [24600/70000], Loss: 0.0246\n",
      "Epoch [24700/70000], Loss: 0.0247\n",
      "Epoch [24800/70000], Loss: 0.0245\n",
      "Epoch [24900/70000], Loss: 0.0246\n",
      "Epoch [25000/70000], Loss: 0.0237\n",
      "Epoch [25100/70000], Loss: 0.0238\n",
      "Epoch [25200/70000], Loss: 0.0247\n",
      "Epoch [25300/70000], Loss: 0.0245\n",
      "Epoch [25400/70000], Loss: 0.0243\n",
      "Epoch [25500/70000], Loss: 0.0240\n",
      "Epoch [25600/70000], Loss: 0.0238\n",
      "Epoch [25700/70000], Loss: 0.0250\n",
      "Epoch [25800/70000], Loss: 0.0242\n",
      "Epoch [25900/70000], Loss: 0.0240\n",
      "Epoch [26000/70000], Loss: 0.0242\n",
      "Epoch [26100/70000], Loss: 0.0242\n",
      "Epoch [26200/70000], Loss: 0.0237\n",
      "Epoch [26300/70000], Loss: 0.0238\n",
      "Epoch [26400/70000], Loss: 0.0244\n",
      "Epoch [26500/70000], Loss: 0.0241\n",
      "Epoch [26600/70000], Loss: 0.0243\n",
      "Epoch [26700/70000], Loss: 0.0241\n",
      "Epoch [26800/70000], Loss: 0.0244\n",
      "Epoch [26900/70000], Loss: 0.0243\n",
      "Epoch [27000/70000], Loss: 0.0241\n",
      "Epoch [27100/70000], Loss: 0.0244\n",
      "Epoch [27200/70000], Loss: 0.0239\n",
      "Epoch [27300/70000], Loss: 0.0245\n",
      "Epoch [27400/70000], Loss: 0.0238\n",
      "Epoch [27500/70000], Loss: 0.0247\n",
      "Epoch [27600/70000], Loss: 0.0248\n",
      "Epoch [27700/70000], Loss: 0.0239\n",
      "Epoch [27800/70000], Loss: 0.0238\n",
      "Epoch [27900/70000], Loss: 0.0238\n",
      "Epoch [28000/70000], Loss: 0.0239\n",
      "Epoch [28100/70000], Loss: 0.0238\n",
      "Epoch [28200/70000], Loss: 0.0247\n",
      "Epoch [28300/70000], Loss: 0.0245\n",
      "Epoch [28400/70000], Loss: 0.0239\n",
      "Epoch [28500/70000], Loss: 0.0239\n",
      "Epoch [28600/70000], Loss: 0.0248\n",
      "Epoch [28700/70000], Loss: 0.0237\n",
      "Epoch [28800/70000], Loss: 0.0239\n",
      "Epoch [28900/70000], Loss: 0.0237\n",
      "Epoch [29000/70000], Loss: 0.0238\n",
      "Epoch [29100/70000], Loss: 0.0248\n",
      "Epoch [29200/70000], Loss: 0.0245\n",
      "Epoch [29300/70000], Loss: 0.0242\n",
      "Epoch [29400/70000], Loss: 0.0239\n",
      "Epoch [29500/70000], Loss: 0.0240\n",
      "Epoch [29600/70000], Loss: 0.0239\n",
      "Epoch [29700/70000], Loss: 0.0239\n",
      "Epoch [29800/70000], Loss: 0.0238\n",
      "Epoch [29900/70000], Loss: 0.0244\n",
      "Epoch [30000/70000], Loss: 0.0240\n",
      "Epoch [30100/70000], Loss: 0.0237\n",
      "Epoch [30200/70000], Loss: 0.0246\n",
      "Epoch [30300/70000], Loss: 0.0239\n",
      "Epoch [30400/70000], Loss: 0.0247\n",
      "Epoch [30500/70000], Loss: 0.0238\n",
      "Epoch [30600/70000], Loss: 0.0237\n",
      "Epoch [30700/70000], Loss: 0.0237\n",
      "Epoch [30800/70000], Loss: 0.0244\n",
      "Epoch [30900/70000], Loss: 0.0238\n",
      "Epoch [31000/70000], Loss: 0.0237\n",
      "Epoch [31100/70000], Loss: 0.0248\n",
      "Epoch [31200/70000], Loss: 0.0246\n",
      "Epoch [31300/70000], Loss: 0.0244\n",
      "Epoch [31400/70000], Loss: 0.0243\n",
      "Epoch [31500/70000], Loss: 0.0246\n",
      "Epoch [31600/70000], Loss: 0.0237\n",
      "Epoch [31700/70000], Loss: 0.0241\n",
      "Epoch [31800/70000], Loss: 0.0247\n",
      "Epoch [31900/70000], Loss: 0.0243\n",
      "Epoch [32000/70000], Loss: 0.0238\n",
      "Epoch [32100/70000], Loss: 0.0247\n",
      "Epoch [32200/70000], Loss: 0.0250\n",
      "Epoch [32300/70000], Loss: 0.0237\n",
      "Epoch [32400/70000], Loss: 0.0238\n",
      "Epoch [32500/70000], Loss: 0.0238\n",
      "Epoch [32600/70000], Loss: 0.0243\n",
      "Epoch [32700/70000], Loss: 0.0240\n",
      "Epoch [32800/70000], Loss: 0.0247\n",
      "Epoch [32900/70000], Loss: 0.0240\n",
      "Epoch [33000/70000], Loss: 0.0238\n",
      "Epoch [33100/70000], Loss: 0.0243\n",
      "Epoch [33200/70000], Loss: 0.0243\n",
      "Epoch [33300/70000], Loss: 0.0240\n",
      "Epoch [33400/70000], Loss: 0.0243\n",
      "Epoch [33500/70000], Loss: 0.0244\n",
      "Epoch [33600/70000], Loss: 0.0238\n",
      "Epoch [33700/70000], Loss: 0.0242\n",
      "Epoch [33800/70000], Loss: 0.0245\n",
      "Epoch [33900/70000], Loss: 0.0238\n",
      "Epoch [34000/70000], Loss: 0.0245\n",
      "Epoch [34100/70000], Loss: 0.0239\n",
      "Epoch [34200/70000], Loss: 0.0238\n",
      "Epoch [34300/70000], Loss: 0.0239\n",
      "Epoch [34400/70000], Loss: 0.0243\n",
      "Epoch [34500/70000], Loss: 0.0245\n",
      "Epoch [34600/70000], Loss: 0.0237\n",
      "Epoch [34700/70000], Loss: 0.0248\n",
      "Epoch [34800/70000], Loss: 0.0237\n",
      "Epoch [34900/70000], Loss: 0.0247\n",
      "Epoch [35000/70000], Loss: 0.0241\n",
      "Epoch [35100/70000], Loss: 0.0244\n",
      "Epoch [35200/70000], Loss: 0.0245\n",
      "Epoch [35300/70000], Loss: 0.0239\n",
      "Epoch [35400/70000], Loss: 0.0238\n",
      "Epoch [35500/70000], Loss: 0.0248\n",
      "Epoch [35600/70000], Loss: 0.0238\n",
      "Epoch [35700/70000], Loss: 0.0243\n",
      "Epoch [35800/70000], Loss: 0.0241\n",
      "Epoch [35900/70000], Loss: 0.0240\n",
      "Epoch [36000/70000], Loss: 0.0244\n",
      "Epoch [36100/70000], Loss: 0.0238\n",
      "Epoch [36200/70000], Loss: 0.0249\n",
      "Epoch [36300/70000], Loss: 0.0238\n",
      "Epoch [36400/70000], Loss: 0.0238\n",
      "Epoch [36500/70000], Loss: 0.0248\n",
      "Epoch [36600/70000], Loss: 0.0244\n",
      "Epoch [36700/70000], Loss: 0.0238\n",
      "Epoch [36800/70000], Loss: 0.0237\n",
      "Epoch [36900/70000], Loss: 0.0246\n",
      "Epoch [37000/70000], Loss: 0.0243\n",
      "Epoch [37100/70000], Loss: 0.0247\n",
      "Epoch [37200/70000], Loss: 0.0241\n",
      "Epoch [37300/70000], Loss: 0.0238\n",
      "Epoch [37400/70000], Loss: 0.0246\n",
      "Epoch [37500/70000], Loss: 0.0243\n",
      "Epoch [37600/70000], Loss: 0.0239\n",
      "Epoch [37700/70000], Loss: 0.0242\n",
      "Epoch [37800/70000], Loss: 0.0238\n",
      "Epoch [37900/70000], Loss: 0.0239\n",
      "Epoch [38000/70000], Loss: 0.0239\n",
      "Epoch [38100/70000], Loss: 0.0238\n",
      "Epoch [38200/70000], Loss: 0.0246\n",
      "Epoch [38300/70000], Loss: 0.0245\n",
      "Epoch [38400/70000], Loss: 0.0239\n",
      "Epoch [38500/70000], Loss: 0.0249\n",
      "Epoch [38600/70000], Loss: 0.0238\n",
      "Epoch [38700/70000], Loss: 0.0244\n",
      "Epoch [38800/70000], Loss: 0.0239\n",
      "Epoch [38900/70000], Loss: 0.0242\n",
      "Epoch [39000/70000], Loss: 0.0239\n",
      "Epoch [39100/70000], Loss: 0.0244\n",
      "Epoch [39200/70000], Loss: 0.0247\n",
      "Epoch [39300/70000], Loss: 0.0238\n",
      "Epoch [39400/70000], Loss: 0.0240\n",
      "Epoch [39500/70000], Loss: 0.0237\n",
      "Epoch [39600/70000], Loss: 0.0248\n",
      "Epoch [39700/70000], Loss: 0.0238\n",
      "Epoch [39800/70000], Loss: 0.0246\n",
      "Epoch [39900/70000], Loss: 0.0239\n",
      "Epoch [40000/70000], Loss: 0.0238\n",
      "Epoch [40100/70000], Loss: 0.0238\n",
      "Epoch [40200/70000], Loss: 0.0241\n",
      "Epoch [40300/70000], Loss: 0.0243\n",
      "Epoch [40400/70000], Loss: 0.0242\n",
      "Epoch [40500/70000], Loss: 0.0248\n",
      "Epoch [40600/70000], Loss: 0.0245\n",
      "Epoch [40700/70000], Loss: 0.0240\n",
      "Epoch [40800/70000], Loss: 0.0241\n",
      "Epoch [40900/70000], Loss: 0.0242\n",
      "Epoch [41000/70000], Loss: 0.0255\n",
      "Epoch [41100/70000], Loss: 0.0247\n",
      "Epoch [41200/70000], Loss: 0.0249\n",
      "Epoch [41300/70000], Loss: 0.0243\n",
      "Epoch [41400/70000], Loss: 0.0240\n",
      "Epoch [41500/70000], Loss: 0.0245\n",
      "Epoch [41600/70000], Loss: 0.0238\n",
      "Epoch [41700/70000], Loss: 0.0248\n",
      "Epoch [41800/70000], Loss: 0.0243\n",
      "Epoch [41900/70000], Loss: 0.0241\n",
      "Epoch [42000/70000], Loss: 0.0244\n",
      "Epoch [42100/70000], Loss: 0.0238\n",
      "Epoch [42200/70000], Loss: 0.0248\n",
      "Epoch [42300/70000], Loss: 0.0238\n",
      "Epoch [42400/70000], Loss: 0.0238\n",
      "Epoch [42500/70000], Loss: 0.0242\n",
      "Epoch [42600/70000], Loss: 0.0241\n",
      "Epoch [42700/70000], Loss: 0.0241\n",
      "Epoch [42800/70000], Loss: 0.0238\n",
      "Epoch [42900/70000], Loss: 0.0246\n",
      "Epoch [43000/70000], Loss: 0.0247\n",
      "Epoch [43100/70000], Loss: 0.0250\n",
      "Epoch [43200/70000], Loss: 0.0247\n",
      "Epoch [43300/70000], Loss: 0.0238\n",
      "Epoch [43400/70000], Loss: 0.0242\n",
      "Epoch [43500/70000], Loss: 0.0243\n",
      "Epoch [43600/70000], Loss: 0.0238\n",
      "Epoch [43700/70000], Loss: 0.0243\n",
      "Epoch [43800/70000], Loss: 0.0238\n",
      "Epoch [43900/70000], Loss: 0.0239\n",
      "Epoch [44000/70000], Loss: 0.0238\n",
      "Epoch [44100/70000], Loss: 0.0239\n",
      "Epoch [44200/70000], Loss: 0.0242\n",
      "Epoch [44300/70000], Loss: 0.0246\n",
      "Epoch [44400/70000], Loss: 0.0246\n",
      "Epoch [44500/70000], Loss: 0.0250\n",
      "Epoch [44600/70000], Loss: 0.0238\n",
      "Epoch [44700/70000], Loss: 0.0247\n",
      "Epoch [44800/70000], Loss: 0.0241\n",
      "Epoch [44900/70000], Loss: 0.0244\n",
      "Epoch [45000/70000], Loss: 0.0243\n",
      "Epoch [45100/70000], Loss: 0.0241\n",
      "Epoch [45200/70000], Loss: 0.0238\n",
      "Epoch [45300/70000], Loss: 0.0245\n",
      "Epoch [45400/70000], Loss: 0.0245\n",
      "Epoch [45500/70000], Loss: 0.0244\n",
      "Epoch [45600/70000], Loss: 0.0239\n",
      "Epoch [45700/70000], Loss: 0.0246\n",
      "Epoch [45800/70000], Loss: 0.0238\n",
      "Epoch [45900/70000], Loss: 0.0239\n",
      "Epoch [46000/70000], Loss: 0.0248\n",
      "Epoch [46100/70000], Loss: 0.0238\n",
      "Epoch [46200/70000], Loss: 0.0246\n",
      "Epoch [46300/70000], Loss: 0.0241\n",
      "Epoch [46400/70000], Loss: 0.0246\n",
      "Epoch [46500/70000], Loss: 0.0249\n",
      "Epoch [46600/70000], Loss: 0.0248\n",
      "Epoch [46700/70000], Loss: 0.0246\n",
      "Epoch [46800/70000], Loss: 0.0239\n",
      "Epoch [46900/70000], Loss: 0.0243\n",
      "Epoch [47000/70000], Loss: 0.0240\n",
      "Epoch [47100/70000], Loss: 0.0238\n",
      "Epoch [47200/70000], Loss: 0.0241\n",
      "Epoch [47300/70000], Loss: 0.0246\n",
      "Epoch [47400/70000], Loss: 0.0243\n",
      "Epoch [47500/70000], Loss: 0.0246\n",
      "Epoch [47600/70000], Loss: 0.0246\n",
      "Epoch [47700/70000], Loss: 0.0238\n",
      "Epoch [47800/70000], Loss: 0.0244\n",
      "Epoch [47900/70000], Loss: 0.0241\n",
      "Epoch [48000/70000], Loss: 0.0245\n",
      "Epoch [48100/70000], Loss: 0.0250\n",
      "Epoch [48200/70000], Loss: 0.0238\n",
      "Epoch [48300/70000], Loss: 0.0242\n",
      "Epoch [48400/70000], Loss: 0.0240\n",
      "Epoch [48500/70000], Loss: 0.0244\n",
      "Epoch [48600/70000], Loss: 0.0239\n",
      "Epoch [48700/70000], Loss: 0.0240\n",
      "Epoch [48800/70000], Loss: 0.0249\n",
      "Epoch [48900/70000], Loss: 0.0242\n",
      "Epoch [49000/70000], Loss: 0.0239\n",
      "Epoch [49100/70000], Loss: 0.0240\n",
      "Epoch [49200/70000], Loss: 0.0244\n",
      "Epoch [49300/70000], Loss: 0.0240\n",
      "Epoch [49400/70000], Loss: 0.0243\n",
      "Epoch [49500/70000], Loss: 0.0239\n",
      "Epoch [49600/70000], Loss: 0.0241\n",
      "Epoch [49700/70000], Loss: 0.0245\n",
      "Epoch [49800/70000], Loss: 0.0241\n",
      "Epoch [49900/70000], Loss: 0.0243\n",
      "Epoch [50000/70000], Loss: 0.0245\n",
      "Epoch [50100/70000], Loss: 0.0246\n",
      "Epoch [50200/70000], Loss: 0.0241\n",
      "Epoch [50300/70000], Loss: 0.0244\n",
      "Epoch [50400/70000], Loss: 0.0241\n",
      "Epoch [50500/70000], Loss: 0.0238\n",
      "Epoch [50600/70000], Loss: 0.0247\n",
      "Epoch [50700/70000], Loss: 0.0243\n",
      "Epoch [50800/70000], Loss: 0.0243\n",
      "Epoch [50900/70000], Loss: 0.0244\n",
      "Epoch [51000/70000], Loss: 0.0244\n",
      "Epoch [51100/70000], Loss: 0.0238\n",
      "Epoch [51200/70000], Loss: 0.0248\n",
      "Epoch [51300/70000], Loss: 0.0250\n",
      "Epoch [51400/70000], Loss: 0.0240\n",
      "Epoch [51500/70000], Loss: 0.0240\n",
      "Epoch [51600/70000], Loss: 0.0243\n",
      "Epoch [51700/70000], Loss: 0.0243\n",
      "Epoch [51800/70000], Loss: 0.0245\n",
      "Epoch [51900/70000], Loss: 0.0243\n",
      "Epoch [52000/70000], Loss: 0.0242\n",
      "Epoch [52100/70000], Loss: 0.0241\n",
      "Epoch [52200/70000], Loss: 0.0239\n",
      "Epoch [52300/70000], Loss: 0.0238\n",
      "Epoch [52400/70000], Loss: 0.0244\n",
      "Epoch [52500/70000], Loss: 0.0243\n",
      "Epoch [52600/70000], Loss: 0.0238\n",
      "Epoch [52700/70000], Loss: 0.0239\n",
      "Epoch [52800/70000], Loss: 0.0238\n",
      "Epoch [52900/70000], Loss: 0.0238\n",
      "Epoch [53000/70000], Loss: 0.0240\n",
      "Epoch [53100/70000], Loss: 0.0250\n",
      "Epoch [53200/70000], Loss: 0.0239\n",
      "Epoch [53300/70000], Loss: 0.0240\n",
      "Epoch [53400/70000], Loss: 0.0241\n",
      "Epoch [53500/70000], Loss: 0.0238\n",
      "Epoch [53600/70000], Loss: 0.0244\n",
      "Epoch [53700/70000], Loss: 0.0244\n",
      "Epoch [53800/70000], Loss: 0.0240\n",
      "Epoch [53900/70000], Loss: 0.0239\n",
      "Epoch [54000/70000], Loss: 0.0246\n",
      "Epoch [54100/70000], Loss: 0.0241\n",
      "Epoch [54200/70000], Loss: 0.0238\n",
      "Epoch [54300/70000], Loss: 0.0239\n",
      "Epoch [54400/70000], Loss: 0.0239\n",
      "Epoch [54500/70000], Loss: 0.0240\n",
      "Epoch [54600/70000], Loss: 0.0246\n",
      "Epoch [54700/70000], Loss: 0.0240\n",
      "Epoch [54800/70000], Loss: 0.0243\n",
      "Epoch [54900/70000], Loss: 0.0241\n",
      "Epoch [55000/70000], Loss: 0.0246\n",
      "Epoch [55100/70000], Loss: 0.0243\n",
      "Epoch [55200/70000], Loss: 0.0246\n",
      "Epoch [55300/70000], Loss: 0.0238\n",
      "Epoch [55400/70000], Loss: 0.0255\n",
      "Epoch [55500/70000], Loss: 0.0244\n",
      "Epoch [55600/70000], Loss: 0.0242\n",
      "Epoch [55700/70000], Loss: 0.0242\n",
      "Epoch [55800/70000], Loss: 0.0239\n",
      "Epoch [55900/70000], Loss: 0.0249\n",
      "Epoch [56000/70000], Loss: 0.0246\n",
      "Epoch [56100/70000], Loss: 0.0242\n",
      "Epoch [56200/70000], Loss: 0.0245\n",
      "Epoch [56300/70000], Loss: 0.0247\n",
      "Epoch [56400/70000], Loss: 0.0244\n",
      "Epoch [56500/70000], Loss: 0.0247\n",
      "Epoch [56600/70000], Loss: 0.0242\n",
      "Epoch [56700/70000], Loss: 0.0245\n",
      "Epoch [56800/70000], Loss: 0.0240\n",
      "Epoch [56900/70000], Loss: 0.0245\n",
      "Epoch [57000/70000], Loss: 0.0240\n",
      "Epoch [57100/70000], Loss: 0.0238\n",
      "Epoch [57200/70000], Loss: 0.0240\n",
      "Epoch [57300/70000], Loss: 0.0240\n",
      "Epoch [57400/70000], Loss: 0.0248\n",
      "Epoch [57500/70000], Loss: 0.0238\n",
      "Epoch [57600/70000], Loss: 0.0239\n",
      "Epoch [57700/70000], Loss: 0.0245\n",
      "Epoch [57800/70000], Loss: 0.0240\n",
      "Epoch [57900/70000], Loss: 0.0246\n",
      "Epoch [58000/70000], Loss: 0.0243\n",
      "Epoch [58100/70000], Loss: 0.0246\n",
      "Epoch [58200/70000], Loss: 0.0245\n",
      "Epoch [58300/70000], Loss: 0.0244\n",
      "Epoch [58400/70000], Loss: 0.0244\n",
      "Epoch [58500/70000], Loss: 0.0251\n",
      "Epoch [58600/70000], Loss: 0.0238\n",
      "Epoch [58700/70000], Loss: 0.0238\n",
      "Epoch [58800/70000], Loss: 0.0238\n",
      "Epoch [58900/70000], Loss: 0.0239\n",
      "Epoch [59000/70000], Loss: 0.0246\n",
      "Epoch [59100/70000], Loss: 0.0246\n",
      "Epoch [59200/70000], Loss: 0.0242\n",
      "Epoch [59300/70000], Loss: 0.0247\n",
      "Epoch [59400/70000], Loss: 0.0241\n",
      "Epoch [59500/70000], Loss: 0.0251\n",
      "Epoch [59600/70000], Loss: 0.0246\n",
      "Epoch [59700/70000], Loss: 0.0242\n",
      "Epoch [59800/70000], Loss: 0.0248\n",
      "Epoch [59900/70000], Loss: 0.0239\n",
      "Epoch [60000/70000], Loss: 0.0251\n",
      "Epoch [60100/70000], Loss: 0.0251\n",
      "Epoch [60200/70000], Loss: 0.0248\n",
      "Epoch [60300/70000], Loss: 0.0244\n",
      "Epoch [60400/70000], Loss: 0.0238\n",
      "Epoch [60500/70000], Loss: 0.0241\n",
      "Epoch [60600/70000], Loss: 0.0238\n",
      "Epoch [60700/70000], Loss: 0.0238\n",
      "Epoch [60800/70000], Loss: 0.0238\n",
      "Epoch [60900/70000], Loss: 0.0240\n",
      "Epoch [61000/70000], Loss: 0.0244\n",
      "Epoch [61100/70000], Loss: 0.0248\n",
      "Epoch [61200/70000], Loss: 0.0241\n",
      "Epoch [61300/70000], Loss: 0.0247\n",
      "Epoch [61400/70000], Loss: 0.0245\n",
      "Epoch [61500/70000], Loss: 0.0238\n",
      "Epoch [61600/70000], Loss: 0.0240\n",
      "Epoch [61700/70000], Loss: 0.0245\n",
      "Epoch [61800/70000], Loss: 0.0239\n",
      "Epoch [61900/70000], Loss: 0.0241\n",
      "Epoch [62000/70000], Loss: 0.0241\n",
      "Epoch [62100/70000], Loss: 0.0248\n",
      "Epoch [62200/70000], Loss: 0.0244\n",
      "Epoch [62300/70000], Loss: 0.0247\n",
      "Epoch [62400/70000], Loss: 0.0246\n",
      "Epoch [62500/70000], Loss: 0.0246\n",
      "Epoch [62600/70000], Loss: 0.0248\n",
      "Epoch [62700/70000], Loss: 0.0238\n",
      "Epoch [62800/70000], Loss: 0.0247\n",
      "Epoch [62900/70000], Loss: 0.0239\n",
      "Epoch [63000/70000], Loss: 0.0246\n",
      "Epoch [63100/70000], Loss: 0.0244\n",
      "Epoch [63200/70000], Loss: 0.0238\n",
      "Epoch [63300/70000], Loss: 0.0238\n",
      "Epoch [63400/70000], Loss: 0.0243\n",
      "Epoch [63500/70000], Loss: 0.0245\n",
      "Epoch [63600/70000], Loss: 0.0238\n",
      "Epoch [63700/70000], Loss: 0.0238\n",
      "Epoch [63800/70000], Loss: 0.0239\n",
      "Epoch [63900/70000], Loss: 0.0251\n",
      "Epoch [64000/70000], Loss: 0.0238\n",
      "Epoch [64100/70000], Loss: 0.0245\n",
      "Epoch [64200/70000], Loss: 0.0252\n",
      "Epoch [64300/70000], Loss: 0.0238\n",
      "Epoch [64400/70000], Loss: 0.0240\n",
      "Epoch [64500/70000], Loss: 0.0239\n",
      "Epoch [64600/70000], Loss: 0.0247\n",
      "Epoch [64700/70000], Loss: 0.0249\n",
      "Epoch [64800/70000], Loss: 0.0238\n",
      "Epoch [64900/70000], Loss: 0.0238\n",
      "Epoch [65000/70000], Loss: 0.0239\n",
      "Epoch [65100/70000], Loss: 0.0239\n",
      "Epoch [65200/70000], Loss: 0.0239\n",
      "Epoch [65300/70000], Loss: 0.0240\n",
      "Epoch [65400/70000], Loss: 0.0240\n",
      "Epoch [65500/70000], Loss: 0.0239\n",
      "Epoch [65600/70000], Loss: 0.0239\n",
      "Epoch [65700/70000], Loss: 0.0240\n",
      "Epoch [65800/70000], Loss: 0.0245\n",
      "Epoch [65900/70000], Loss: 0.0245\n",
      "Epoch [66000/70000], Loss: 0.0242\n",
      "Epoch [66100/70000], Loss: 0.0248\n",
      "Epoch [66200/70000], Loss: 0.0242\n",
      "Epoch [66300/70000], Loss: 0.0240\n",
      "Epoch [66400/70000], Loss: 0.0239\n",
      "Epoch [66500/70000], Loss: 0.0239\n",
      "Epoch [66600/70000], Loss: 0.0238\n",
      "Epoch [66700/70000], Loss: 0.0247\n",
      "Epoch [66800/70000], Loss: 0.0245\n",
      "Epoch [66900/70000], Loss: 0.0242\n",
      "Epoch [67000/70000], Loss: 0.0239\n",
      "Epoch [67100/70000], Loss: 0.0239\n",
      "Epoch [67200/70000], Loss: 0.0238\n",
      "Epoch [67300/70000], Loss: 0.0252\n",
      "Epoch [67400/70000], Loss: 0.0243\n",
      "Epoch [67500/70000], Loss: 0.0250\n",
      "Epoch [67600/70000], Loss: 0.0245\n",
      "Epoch [67700/70000], Loss: 0.0238\n",
      "Epoch [67800/70000], Loss: 0.0240\n",
      "Epoch [67900/70000], Loss: 0.0244\n",
      "Epoch [68000/70000], Loss: 0.0238\n",
      "Epoch [68100/70000], Loss: 0.0246\n",
      "Epoch [68200/70000], Loss: 0.0245\n",
      "Epoch [68300/70000], Loss: 0.0239\n",
      "Epoch [68400/70000], Loss: 0.0239\n",
      "Epoch [68500/70000], Loss: 0.0242\n",
      "Epoch [68600/70000], Loss: 0.0239\n",
      "Epoch [68700/70000], Loss: 0.0248\n",
      "Epoch [68800/70000], Loss: 0.0243\n",
      "Epoch [68900/70000], Loss: 0.0247\n",
      "Epoch [69000/70000], Loss: 0.0241\n",
      "Epoch [69100/70000], Loss: 0.0242\n",
      "Epoch [69200/70000], Loss: 0.0247\n",
      "Epoch [69300/70000], Loss: 0.0240\n",
      "Epoch [69400/70000], Loss: 0.0244\n",
      "Epoch [69500/70000], Loss: 0.0239\n",
      "Epoch [69600/70000], Loss: 0.0240\n",
      "Epoch [69700/70000], Loss: 0.0239\n",
      "Epoch [69800/70000], Loss: 0.0244\n",
      "Epoch [69900/70000], Loss: 0.0238\n",
      "Epoch [70000/70000], Loss: 0.0239\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, optimizer, and KLDivLoss function\n",
    "input_size = X_train_day2.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = y_train_prob.shape[1]\n",
    "\n",
    "model = SoftLabelNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')  # KLDivLoss for comparing distributions\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "# Instantiate the Trainer class and start training\n",
    "trainer = Trainer(model, optimizer, criterion, X_train_day2, y_train_prob, num_epochs=70000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69, 64]), torch.Size([69, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_day2.shape, y_test_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1337, 64]), torch.Size([1337, 3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_day2.shape, y_train_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence on test set: 2.2843\n"
     ]
    }
   ],
   "source": [
    "kl_divergence = trainer.evaluate_kl_divergence(X_test_day2, y_test_prob)\n",
    "print(f\"KL Divergence on test set: {kl_divergence:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-env-arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

'''
Version 3
1. updated with the feature representation generated by the base encoder
2. updated the new dataloader that can return the lineage information
3. updated with the feature representation of total 41201 cells generated by the base encoder
4. change the optimizer from Adam to AdamW
5. add an additional figure: train_config.batch_seed to control the batch generator 
'''

# general package
import tempfile
import os
import numpy as np

# deep learning package
import torch
import torchvision.models as models
import numpy as np
import os
import torchvision.transforms as T
import torch.nn as nn
import torch.nn.functional as F
from torch.multiprocessing import cpu_count
import torchvision.transforms as T

import pytorch_lightning as pl
import torch.nn.functional as F
from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR
from torch.optim import Adam
from torch.utils.data import TensorDataset


import time
start_time = time.time()
print("start time:", start_time)

def default(val, def_val):
    return def_val if val is None else val

def reproducibility(config):
    SEED = int(config.seed)
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(SEED)
    if (config.cuda):
        torch.cuda.manual_seed(SEED)


def device_as(t1, t2):
    """
    Moves t1 to the device of t2
    """
    return t1.to(t2.device)

# From https://github.com/PyTorchLightning/pytorch-lightning/issues/924
def weights_update(model, checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    model_dict = model.state_dict()
    pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in model_dict}
    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)
    print(f'Checkpoint {checkpoint_path} was loaded')
    return model



class ContrastiveLoss(nn.Module):
    """
    Vanilla Contrastive loss, also called InfoNceLoss as in SimCLR paper
    """
    def __init__(self, batch_size, temperature=0.5):
        super().__init__()
        self.batch_size = batch_size
        self.temperature = temperature
        self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float()

    def calc_similarity_batch(self, a, b):
        representations = torch.cat([a, b], dim=0)
        return F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)

    def forward(self, proj_1, proj_2):
        """
        proj_1 and proj_2 are batched embeddings [batch, embedding_dim]
        where corresponding indices are pairs
        z_i, z_j in the SimCLR paper
        """
        batch_size = proj_1.shape[0]
        z_i = F.normalize(proj_1, p=2, dim=1)
        z_j = F.normalize(proj_2, p=2, dim=1)

        similarity_matrix = self.calc_similarity_batch(z_i, z_j)

        sim_ij = torch.diag(similarity_matrix, batch_size)
        sim_ji = torch.diag(similarity_matrix, -batch_size)

        positives = torch.cat([sim_ij, sim_ji], dim=0)

        nominator = torch.exp(positives / self.temperature)

        denominator = device_as(self.mask, similarity_matrix) * torch.exp(similarity_matrix / self.temperature)

        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))
        loss = torch.sum(all_losses) / (2 * self.batch_size)
        return loss


#-------------------------------------------------------------------------------------------------------------------
"""Add projection Head for embedding"""
### resnetをmlpに置き換える。

class AddProjectionMLP(nn.Module):
    def __init__(self, config):

        """
        input_dim: The size of the input features. 
        hidden_dims: A list of integers where each integer specifies the number of neurons in that hidden layer.
        embedding_size: The size of the output from the projection head.
        """
        super(AddProjectionMLP, self).__init__()

        # Define the MLP as the base encoder
        input_dim = config.input_dim
        hidden_dims = config.hidden_dims
        embedding_size = config.embedding_size

        layers = []
        for i in range(len(hidden_dims)):
            layers.append(nn.Linear(input_dim if i == 0 else hidden_dims[i-1], hidden_dims[i]))
            layers.append(nn.BatchNorm1d(hidden_dims[i]))
            layers.append(nn.ReLU(inplace=True))
        self.base_encoder = nn.Sequential(*layers)

        # Define the projection head
        self.projection = nn.Sequential(
            nn.Linear(in_features=hidden_dims[-1], out_features=hidden_dims[-1]),
            nn.BatchNorm1d(hidden_dims[-1]),
            nn.ReLU(),
            nn.Linear(in_features=hidden_dims[-1], out_features=embedding_size),
            nn.BatchNorm1d(embedding_size),
        )

    def forward(self, x, return_embedding=False):
        # Flatten the input if necessary
        x = x.view(x.size(0), -1)
        embedding = self.base_encoder(x)
        if return_embedding:
            return embedding
        return self.projection(embedding)

    # extract the features geenerated by the base encoder 
    def get_features(self, x):
        """
        Extracts features from the base encoder.
        """
        x = x.view(x.size(0), -1)  # Flatten the input if necessary
        features = self.base_encoder(x)
        return features


#-------------------------------------------------------------------------------------------------------------------


def define_param_groups(model, weight_decay, optimizer_name):
    def exclude_from_wd_and_adaptation(name):
        if 'bn' in name:
            return True
        if optimizer_name == 'lars' and 'bias' in name:
            return True

    param_groups = [
        {
            'params': [p for name, p in model.named_parameters() if not exclude_from_wd_and_adaptation(name)],
            'weight_decay': weight_decay,
            'layer_adaptation': True,
        },
        {
            'params': [p for name, p in model.named_parameters() if exclude_from_wd_and_adaptation(name)],
            'weight_decay': 0.,
            'layer_adaptation': False,
        },
    ]
    return param_groups


# class SimCLR_pl(pl.LightningModule):
#     def __init__(self, config):
#         super().__init__()
#         self.config = config

#         #self.model = AddProjection(config, model=model, mlp_dim=feat_dim)
#         self.model = AddProjectionMLP(config)

#         self.loss = ContrastiveLoss(config.batch_size, temperature=self.config.temperature)

#     def forward(self, X):
#         return self.model(X)

#     def training_step(self, batch, batch_idx):
#         # print(batch)
#         x1, x2= batch
#         z1 = self.model(x1)
#         z2 = self.model(x2)
#         loss = self.loss(z1, z2)
#         self.log('Contrastive loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
#         return loss

#     def configure_optimizers(self):
#         max_epochs = int(self.config.epochs)
#         param_groups = define_param_groups(self.model, self.config.weight_decay, 'adam')
#         lr = self.config.lr
#         optimizer = Adam(param_groups, lr=lr, weight_decay=self.config.weight_decay)

#         print(f'Optimizer Adam, '
#               f'Learning Rate {lr}, '
#               f'Effective batch size {self.config.batch_size * self.config.gradient_accumulation_steps}')

#         scheduler_warmup = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=max_epochs,
#                                                          warmup_start_lr=0.0)

#         return [optimizer], [scheduler_warmup]


from torch.optim import AdamW  # Import AdamW optimizer

class SimCLR_pl(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.model = AddProjectionMLP(config)
        self.loss = ContrastiveLoss(config.batch_size, temperature=self.config.temperature)

    def forward(self, X):
        return self.model(X)

    def training_step(self, batch, batch_idx):
        x1, x2 = batch
        z1 = self.model(x1)
        z2 = self.model(x2)
        loss = self.loss(z1, z2)
        self.log('Contrastive loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
        max_epochs = int(self.config.epochs)
        param_groups = define_param_groups(self.model, self.config.weight_decay, 'adamw')  # Note the change here
        lr = self.config.lr
        # Change optimizer to AdamW
        optimizer = AdamW(param_groups, lr=lr, weight_decay=self.config.weight_decay)

        print(f'Optimizer AdamW, '  # Make sure to change the print statement accordingly
              f'Learning Rate {lr}, '
              f'Effective batch size {self.config.batch_size * self.config.gradient_accumulation_steps}')

        scheduler_warmup = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=max_epochs,
                                                         warmup_start_lr=0.0)

        return [optimizer], [scheduler_warmup]



#-------------------------------------------------------------------------------------------------------------------
#--------------------------------------------------Training Step----------------------------------------------------
"""## Hyperparameters, and configuration stuff"""

# a lazy way to pass the config file
class Hparams:
    def __init__(self):
        self.input_dim = 2000 #number of genes
        self.hidden_dims = [1024, 512, 256, 128, 64] # [1024, 512, 256, 128, 30] want to be 30
        self.embedding_size = 32 #50 (128 for image contrastive learn)
        
        self.epochs = 220 # number of training epochs 300 before changing to 15
        self.seed = 20 # randomness seed
        self.cuda = True # use nvidia gpu
        self.save = "./saved_models/" # save checkpoint
        self.load = False # load pretrained checkpoint
        self.gradient_accumulation_steps = 5 # gradient accumulation steps
        self.batch_size = 25 # should be 2*10 in each batch. 
        self.lr = 3e-4 # for ADAm only
        self.weight_decay = 1e-6
        self.temperature = 0.5 # 0.1 or 0.5 #0.1 1 10 100 ...
        self.batch_seed = 17
        self.checkpoint_path = './scContrastiveLearn.ckpt' # replace checkpoint path here
#-------------------------------------------------------------------------------------------------------------------


"""## Pretraining main logic"""

from pytorch_lightning import Trainer
import os
from pytorch_lightning.callbacks import GradientAccumulationScheduler
from pytorch_lightning.callbacks import ModelCheckpoint
# from torchvision.models import  resnet18


available_gpus = len([torch.cuda.device(i) for i in range(torch.cuda.device_count())])
save_model_path = os.path.join(os.getcwd(), "saved_models/")
print('available_gpus:',available_gpus)
filename='scContrastiveLearn_adam_'
resume_from_checkpoint = False
train_config = Hparams()

reproducibility(train_config)
save_name = filename + '.ckpt'
#-------------------------------------------------------------------------------------------------------------------

model = SimCLR_pl(train_config)
#-------------------------------------------------DataLoading-------------------------------------------------------
import Larry_Dataloader as LD
out_dir = "/home/users/syang71/kzlinlab/projects/scContrastiveLearn/out/joshua/data/feat_324_AdamW/"

larry_dataset, lineage_info = LD.Larry_DataLoader(train_config.input_dim,train_config.batch_size, train_config.batch_seed)
data_loader = torch.utils.data.DataLoader(dataset=larry_dataset, batch_size=train_config.batch_size, shuffle=False, num_workers=1) #num_workers=cpu_count()//2

# save the lineage info for UMAP plotting
np.save(out_dir+ 'lineage_info_324_25.npy', lineage_info)
#-------------------------------------------------------------------------------------------------------------------

accumulator = GradientAccumulationScheduler(scheduling={0: train_config.gradient_accumulation_steps})
checkpoint_callback = ModelCheckpoint(filename=filename, dirpath=save_model_path,
                                        save_last=True, save_top_k=2,monitor='Contrastive loss_epoch',mode='min')

if resume_from_checkpoint:
  print("resume from the checkpoint.")
  trainer = Trainer(callbacks=[accumulator, checkpoint_callback],
                  gpus=available_gpus,
                  max_epochs=train_config.epochs,
                  resume_from_checkpoint=train_config.checkpoint_path)
else:
  print("train from the begining.")
  trainer = Trainer(callbacks=[accumulator, checkpoint_callback],
                  gpus=available_gpus,
                  max_epochs=train_config.epochs)


trainer.fit(model, data_loader)


trainer.save_checkpoint(save_name)


#-------------------------------------Extract Features generated by the base encoder-------------------------------
model.eval()  # Set the model to evaluation mode
model.to('cuda' if torch.cuda.is_available() else 'cpu')  # Move model to the appropriate device


##-----------------------Extract Features generated by the base encoder for cell pairs-----------------------------

features_list_X = []
features_list_Y = []
for batch in data_loader:  
    X, Y = batch
    X = X.to(next(model.parameters()).device)  # Ensure X is on the correct device
    Y = Y.to(next(model.parameters()).device)
    with torch.no_grad():  # No need to compute gradients
        batch_features_X = model.model.get_features(X)  # Extract features
        batch_features_Y = model.model.get_features(Y)
    features_list_X.append(batch_features_X.cpu().detach().numpy())  # Store features as NumPy array
    features_list_Y.append(batch_features_Y.cpu().detach().numpy())

# Concatenate all batch features into a single NumPy array
features_X = np.concatenate(features_list_X, axis=0)
features_Y = np.concatenate(features_list_Y, axis=0)

print("Shape of the feature representation generated by the base encoder:", features_X.shape, features_Y.shape)
np.save(out_dir+'scBaseEncoderFeat_X_324_25.npy', features_X)
np.save(out_dir+'scBaseEncoderFeat_Y_324_25.npy', features_Y)


##------------------------------Extract Features generated by the base encoder for cells----------------------------
import anndata as ad

adata_subset = ad.read_h5ad('/home/users/syang71/Dataset/Larry_41201_2000.h5ad')
count_matrix = adata_subset.X
count_matrix_arr = count_matrix.toarray()
count_matrix_th = torch.from_numpy(count_matrix_arr)
dataset_cell = TensorDataset(count_matrix_th)
data_loader_all = torch.utils.data.DataLoader(dataset_cell, batch_size=train_config.batch_size, shuffle=False, num_workers=1,drop_last=False)
print("num of batches for all cells (not cell pairs):", len(data_loader_all))


features_list_Z = []

for batch in data_loader_all:  
    Z = batch[0]
    Z = Z.to(next(model.parameters()).device)  
    with torch.no_grad():  # No need to compute gradients
        batch_features_Z = model.model.get_features(Z)  # Extract features
        
    features_list_Z.append(batch_features_Z.cpu().detach().numpy())  # Store features as NumPy array

# Concatenate all batch features into a single NumPy array
features_Z = np.concatenate(features_list_Z, axis=0)


print("Shape of the feature representation generated by the base encoder:", features_Z.shape)
np.save(out_dir+ 'scBaseEncoderFeat_Z_324_25.npy', features_Z)


#-------------------------------------------------------------------------------------------------------------------

"""## Save only backbone weights from Resnet18 that are only necessary for fine tuning"""
model_pl = SimCLR_pl(train_config)
model_pl = weights_update(model_pl, "scContrastiveLearn_adam_.ckpt")

baseencoder_backbone_weights = model_pl.model.base_encoder.state_dict()
# # Saving base encoder weights
# base_encoder_weights = model_pl.model.base_encoder.state_dict()
# torch.save(base_encoder_weights, 'base_encoder_weights.ckpt')

# # Saving projection head weights
# projection_head_weights = model_pl.model.projection.state_dict()
# torch.save(projection_head_weights, 'projection_head_weights.ckpt')

print(baseencoder_backbone_weights)
torch.save({
            'model_state_dict': baseencoder_backbone_weights,
            }, 'baseencoder_weights.ckpt')

#-------------------------------------------------------------------------------------------------------------------

end_time = time.time()
print("end time:", end_time)
print(f"Execution time: {end_time - start_time} seconds")